"""ARC-AGI Dataset implementation."""
import json
import random
from pathlib import Path
from typing import List

import torch
from torch.utils.data import Dataset

from .augmentation import AugmentationPipeline


class ARCDataset(Dataset):
    """PyTorch Dataset for ARC-AGI tasks.

    Each ARC-AGI task consists of:
    - Training examples: input/output grid pairs demonstrating the pattern
    - Test examples: input grids requiring output prediction

    All grids use values 0-9 representing colors.
    """

    def __init__(self, data_dir: str, split: str = "training"):
        """Initialize ARC-AGI dataset.

        Args:
            data_dir: Path to data/ directory containing the split subdirectory
            split: Subdirectory name (e.g. "training", "evaluation", "train_all")

        Raises:
            FileNotFoundError: If split directory doesn't exist
        """
        self.data_dir = Path(data_dir)
        self.split = split
        split_dir = self.data_dir / split

        if not split_dir.exists():
            raise FileNotFoundError(f"Split directory not found: {split_dir}")

        # Load all tasks eagerly (small dataset, ~800 tasks total)
        json_files = sorted(split_dir.glob("*.json"))
        self.tasks = []

        for json_file in json_files:
            task_id = json_file.stem  # Filename without .json extension
            with open(json_file, "r") as f:
                task_data = json.load(f)
            self.tasks.append((task_id, task_data))

    def __len__(self) -> int:
        """Return number of tasks in dataset."""
        return len(self.tasks)

    def __getitem__(self, idx: int) -> dict:
        """Get a single task by index.

        Args:
            idx: Task index

        Returns:
            Dictionary containing:
                - task_id: str (filename without .json)
                - train_inputs: List[Tensor] - each shape (H, W), dtype=torch.long
                - train_outputs: List[Tensor] - each shape (H, W), dtype=torch.long
                - test_inputs: List[Tensor] - each shape (H, W), dtype=torch.long
                - test_outputs: List[Tensor] - each shape (H, W), dtype=torch.long

            Grid values are 0-9 (10 colors). Grids have variable sizes.
        """
        task_id, task_data = self.tasks[idx]

        # Convert train pairs
        train_inputs = [
            torch.tensor(pair["input"], dtype=torch.long)
            for pair in task_data["train"]
        ]
        train_outputs = [
            torch.tensor(pair["output"], dtype=torch.long)
            for pair in task_data["train"]
        ]

        # Convert test pairs
        test_inputs = [
            torch.tensor(pair["input"], dtype=torch.long)
            for pair in task_data["test"]
        ]
        test_outputs = [
            torch.tensor(pair["output"], dtype=torch.long)
            for pair in task_data["test"]
        ]

        return {
            "task_id": task_id,
            "train_inputs": train_inputs,
            "train_outputs": train_outputs,
            "test_inputs": test_inputs,
            "test_outputs": test_outputs,
        }


class AugmentedARCDataset(Dataset):
    """ARC dataset with on-the-fly augmentation.

    Wraps ARCDataset and applies augmentation during __getitem__.
    Each call to __getitem__ produces a different augmented version,
    effectively creating infinite training data from finite tasks.
    """

    def __init__(
        self,
        data_dir: str,
        split: str = "training",
        enable_d8: bool = True,
        enable_color: bool = True,
        enable_translation: bool = True,
    ):
        """Initialize augmented dataset.

        Args:
            data_dir: Path to data/ directory containing the split subdirectory
            split: Subdirectory name (e.g. "training", "evaluation", "train_all")
            enable_d8: Whether to apply D8 geometric transforms
            enable_color: Whether to apply color permutations (colors 1-9, preserving 0)
            enable_translation: Whether to apply translational augmentation (grid shift)
        """
        self.base_dataset = ARCDataset(data_dir, split)
        self.pipeline = AugmentationPipeline(
            enable_d8=enable_d8,
            enable_color=enable_color,
            enable_translation=enable_translation,
        )

    def __len__(self) -> int:
        """Return number of tasks in dataset."""
        return len(self.base_dataset)

    def __getitem__(self, idx: int) -> dict:
        """Get a single augmented task by index.

        Each call applies FRESH random augmentation, so the same idx
        called multiple times will return different augmented versions.

        Critically, the SAME augmentation (same D8 transform, same color permutation,
        same translation offset) is applied to ALL pairs in the task. This is required
        for in-context learning: consistent mapping across demo pairs lets the model
        infer the transformation rule.

        Args:
            idx: Task index

        Returns:
            Dictionary containing augmented grids with same structure as ARCDataset
        """
        item = self.base_dataset[idx]

        # Sample ONE consistent augmentation for the entire task
        d8, color_perm = self.pipeline.sample()

        # Compute translation padding once across all grids in the task
        pad_top, pad_left = 0, 0
        if self.pipeline.enable_translation:
            all_grids = (
                item["train_inputs"] + item["train_outputs"]
                + item["test_inputs"] + item["test_outputs"]
            )
            if all_grids:
                # Use max(h, w) for both dims to stay safe across D8 rotations
                max_dim = max(max(g.shape[0], g.shape[1]) for g in all_grids)
                avail = max(0, self.pipeline.ARC_MAX - max_dim)
                pad_top = random.randint(0, avail)
                pad_left = random.randint(0, avail)

        def augment_one(inp, out):
            return self.pipeline.apply_with(inp, out, d8, color_perm, pad_top, pad_left)

        augmented_train_inputs = []
        augmented_train_outputs = []
        for inp, out in zip(item["train_inputs"], item["train_outputs"]):
            aug_inp, aug_out = augment_one(inp, out)
            augmented_train_inputs.append(aug_inp)
            augmented_train_outputs.append(aug_out)

        augmented_test_inputs = []
        augmented_test_outputs = []
        for inp, out in zip(item["test_inputs"], item["test_outputs"]):
            aug_inp, aug_out = augment_one(inp, out)
            augmented_test_inputs.append(aug_inp)
            augmented_test_outputs.append(aug_out)

        return {
            "task_id": item["task_id"],
            "train_inputs": augmented_train_inputs,
            "train_outputs": augmented_train_outputs,
            "test_inputs": augmented_test_inputs,
            "test_outputs": augmented_test_outputs,
        }
